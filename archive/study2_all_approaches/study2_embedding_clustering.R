# =============================================================================
# Study 2 (Qualitative): Sentence-Embedding + K-Means Clustering
# SUD Counseling Career Research Project – Focus-group utterances
# -----------------------------------------------------------------------------
# Goal: Identify contextual themes using semantic sentence embeddings instead of
#       traditional bag-of-words LDA.  This approach is better suited to short
#       utterances because the embedding captures whole-sentence meaning.
#
# Workflow ---------------------------------------------------------------
# 1. Load the cleaned substantive utterances generated by
#    `study2_data_preparation.R` (focus_group_substantive.csv).
# 2. Create sentence embeddings with the {text} package (SBERT model
#    "all-mpnet-base-v2").  The first call downloads the model (~400 MB) and
#    caches it for future use.
# 3. Stack embeddings into a matrix and evaluate k-means solutions for k = 2–8
#    using silhouette width.
# 4. Select the k with the highest average silhouette (ties → smallest k).
# 5. Fit final k-means model, assign clusters to utterances.
# 6. Label each cluster with its top 10 tf-idf words.
# 7. Write tidy results to results/r/study2_embedding_clustering/.
# -----------------------------------------------------------------------------
# Author: Auto-generated by AI assistant 2025-06-11
# =============================================================================

# ---- Setup ------------------------------------------------------------------

if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")

pacman::p_load(
  tidyverse,    # data-handling
  tidytext,     # tokenisation / tf-idf
  text,         # Sentence-BERT embeddings
  tidyclust,    # clustering models (tidymodels)
  cluster,      # silhouette
  here          # project-relative paths
)

# Ensure deterministic results
set.seed(123)

# Use dedicated Python env for text embeddings --------------------------------
try({
  reticulate::use_virtualenv("textEnv", required = TRUE)
}, silent = TRUE)

# Directories -----------------------------------------------------------------

data_file    <- here("data", "focus_group_substantive.csv")
results_dir  <- here("results", "r", "study2_embedding_clustering")
if (!dir.exists(results_dir)) dir.create(results_dir, recursive = TRUE)

# ---- 1. Load substantive utterances ----------------------------------------

substantive_data <- read_csv(data_file, show_col_types = FALSE)
cat("Loaded", nrow(substantive_data), "substantive utterances\n")

# ---- 2. Compute sentence embeddings ----------------------------------------
cat("Creating sentence embeddings with SBERT (all-mpnet-base-v2)...\n")

embedding_obj <- textEmbed(substantive_data$cleaned_text,
                           model = "sentence-transformers/all-mpnet-base-v2",
                           layers = "all")
# `textEmbed()` returns a list; $embedding is a matrix [n x 768]
emb_matrix <- embedding_obj$embedding
rownames(emb_matrix) <- substantive_data$response_id

cat("Embedding matrix dimensions:", dim(emb_matrix)[1], "docs x",
    dim(emb_matrix)[2], "features\n")

# ---- 3. Evaluate k-means solutions (k = 2-8) -------------------------------

k_grid <- 2:8
sil_results <- map_dfr(k_grid, function(k) {
  km <- kmeans(emb_matrix, centers = k, nstart = 25, iter.max = 100)
  sil <- silhouette(km$cluster, dist(emb_matrix))[, 3]
  tibble(k = k, avg_silhouette = mean(sil))
})

write_csv(sil_results, file.path(results_dir, "silhouette_by_k.csv"))

best_k <- sil_results %>%
  filter(avg_silhouette == max(avg_silhouette)) %>%
  slice_min(k) %>%            # choose smaller k if tie
  pull(k)

cat("\nSilhouette results:\n")
print(sil_results)
cat("\nOptimal k by silhouette =", best_k, "\n")

# ---- 4. Fit final k-means ----------------------------------------------------

km_final <- kmeans(emb_matrix, centers = best_k, nstart = 50, iter.max = 200)

substantive_data <- substantive_data %>%
  mutate(cluster = km_final$cluster)

write_csv(substantive_data %>% select(response_id, cluster, cleaned_text),
          file.path(results_dir, "utterance_cluster_assignments.csv"))

# ---- 5. Cluster labelling with tf-idf --------------------------------------

cluster_terms <- substantive_data %>%
  unnest_tokens(word, cleaned_text) %>%
  anti_join(stop_words, by = "word") %>%
  count(cluster, word, sort = TRUE) %>%
  bind_tf_idf(word, cluster, n) %>%
  arrange(cluster, desc(tf_idf)) %>%
  group_by(cluster) %>%
  slice_head(n = 10) %>%
  ungroup()

write_csv(cluster_terms, file.path(results_dir, "top_terms_per_cluster.csv"))

# ---- 6. Summary -------------------------------------------------------------

cat("\n=== Sentence-Embedding Clustering Complete ===\n")
cat("Optimal k (silhouette):", best_k, "clusters\n")
cat("Average silhouette:", round(max(sil_results$avg_silhouette), 3), "\n")
cat("Results saved to:", results_dir, "\n") 